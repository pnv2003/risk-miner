{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f8b3605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, try_to_timestamp, row_number, sum as spark_sum\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .appName(\"Apache Jira Issues\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "580437f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "changelog = spark.read.option(\"maxColumns\", 100000).csv(\"./apache/changelog.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# drop all rows with null 'field'\n",
    "changelog_cleaned = changelog.dropna(subset=['field'])\n",
    "\n",
    "# convert 'created' column to timestamp type (invalid formats will become null)\n",
    "changelog_cleaned = changelog_cleaned.withColumn('created', try_to_timestamp(col('created')))\n",
    "\n",
    "# drop rows with null 'created' or 'id'\n",
    "changelog_cleaned = changelog_cleaned.dropna(subset=['created', 'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc95e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial grouping by id (for later merging/aggregation)\n",
    "changelog_issues = changelog_cleaned.groupBy(\"id\").count()\n",
    "\n",
    "# # get the latest change date per id\n",
    "# window_spec = Window.partitionBy('id').orderBy(col('created').desc())\n",
    "# latest_created = changelog_cleaned.withColumn('rn', row_number().over(window_spec)) \\\n",
    "# \t.filter(col('rn') == 1) \\\n",
    "# \t.select('id', col('created').alias('last_change_at'))\n",
    "\n",
    "# # get the earliest change date per id\n",
    "# window_spec_asc = Window.partitionBy('id').orderBy(col('created').asc())\n",
    "# earliest_created = changelog_cleaned.withColumn('rn', row_number().over(window_spec_asc)) \\\n",
    "# \t.filter(col('rn') == 1) \\\n",
    "# \t.select('id', col('created').alias('first_change_at'))\n",
    "\n",
    "# # join the latest and earliest created date to changelog_issues\n",
    "# changelog_issues = changelog_issues \\\n",
    "# \t.join(latest_created, on='id', how='left') \\\n",
    "# \t.join(earliest_created, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9895b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def get_extreme_field_value(df: DataFrame, field_name: str, order: str = \"desc\", select_col: str = \"to\", alias: str = None):\n",
    "    filtered = df.filter(df.field == field_name)\n",
    "    window_spec = Window.partitionBy('key').orderBy(col('created').desc() if order == \"desc\" else col('created').asc())\n",
    "    ranked = filtered.withColumn('rn', row_number().over(window_spec))\n",
    "    selected = ranked.filter(ranked.rn == 1).select('id', select_col)\n",
    "    if alias:\n",
    "        selected = selected.withColumnRenamed(select_col, alias)\n",
    "    return selected\n",
    "\n",
    "# Latest timespent per issue\n",
    "timespent_latest = get_extreme_field_value(changelog_cleaned, 'timespent', order=\"desc\", select_col=\"to\", alias=\"time_spent\")\n",
    "\n",
    "# Latest timeestimate per issue\n",
    "timeestimate_latest = get_extreme_field_value(changelog_cleaned, 'timeestimate', order=\"desc\", select_col=\"to\", alias=\"time_estimate\")\n",
    "\n",
    "# Earliest timeestimate per issue\n",
    "timeestimate_earliest = get_extreme_field_value(changelog_cleaned, 'timeestimate', order=\"asc\", select_col=\"to\", alias=\"original_estimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "909a2086",
   "metadata": {},
   "outputs": [],
   "source": [
    "timespent_latest = timespent_latest.withColumnRenamed('to', 'time_spent')\n",
    "timeestimate_latest = timeestimate_latest.withColumnRenamed('to', 'time_estimate')\n",
    "timeestimate_earliest = timeestimate_earliest.withColumnRenamed('to', 'original_estimate')\n",
    "\n",
    "changelog_issues = changelog_issues \\\n",
    "    .join(timespent_latest, on='id', how='left') \\\n",
    "    .join(timeestimate_latest, on='id', how='left') \\\n",
    "    .join(timeestimate_earliest, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ffee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum, when, col\n",
    "\n",
    "# Count all changes per issue\n",
    "issue_changes = (\n",
    "    changelog_cleaned.groupBy(\"id\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"total_changes\")\n",
    ")\n",
    "\n",
    "# Helper to count changes for a specific field\n",
    "def count_field_changes(df, field_name, alias):\n",
    "    return (\n",
    "        df.withColumn(alias, when(col(\"field\") == field_name, 1).otherwise(0))\n",
    "          .groupBy(\"id\")\n",
    "          .agg(spark_sum(alias).alias(alias))\n",
    "    )\n",
    "\n",
    "# Fields of interest and their aliases\n",
    "fields = [\n",
    "    (\"status\", \"status_changes\"),\n",
    "    (\"priority\", \"priority_changes\"),\n",
    "    (\"assignee\", \"assignee_changes\"),\n",
    "    (\"issuetype\", \"issuetype_changes\"),\n",
    "    (\"resolution\", \"resolution_changes\"),\n",
    "    (\"timeestimate\", \"timeestimate_changes\"),\n",
    "    (\"description\", \"description_changes\"),\n",
    "    (\"Fix Version\", \"fixversion_changes\"),\n",
    "]\n",
    "\n",
    "# Count changes for each field\n",
    "field_change_counts = {\n",
    "    alias: count_field_changes(changelog_cleaned, field, alias)\n",
    "    for field, alias in fields\n",
    "}\n",
    "\n",
    "# Join all counts to issues_final\n",
    "for df in [issue_changes, *field_change_counts.values()]:\n",
    "    changelog_issues = changelog_issues.join(df, on=\"id\", how=\"left\")\n",
    "\n",
    "# Cast counts to integer and fill nulls with 0\n",
    "count_columns = [\n",
    "    \"total_changes\",\n",
    "    \"status_changes\",\n",
    "    \"priority_changes\",\n",
    "    \"assignee_changes\",\n",
    "    \"issuetype_changes\",\n",
    "    \"resolution_changes\",\n",
    "    \"timeestimate_changes\",\n",
    "    \"description_changes\",\n",
    "    \"fixversion_changes\",\n",
    "]\n",
    "\n",
    "for col_name in count_columns:\n",
    "    changelog_issues = changelog_issues.withColumn(\n",
    "        col_name,\n",
    "        when(col(col_name).isNull(), 0).otherwise(col(col_name).cast(\"integer\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71d351a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of reopened issues\n",
    "reopen_times = changelog_cleaned.filter(\n",
    "    (col(\"field\") == \"status\") & (col(\"toString\") == \"Reopened\")\n",
    ").groupBy(\"id\").count().withColumnRenamed(\"count\", \"reopen_times\")\n",
    "\n",
    "# Join reopened count to issues_final\n",
    "changelog_issues = changelog_issues.join(reopen_times, on=\"id\", how=\"left\")\n",
    "\n",
    "# Fill nulls with 0\n",
    "changelog_issues = changelog_issues.withColumn(\n",
    "    \"reopen_times\",\n",
    "    when(col(\"reopen_times\").isNull(), 0).otherwise(col(\"reopen_times\").cast(\"integer\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc7fe670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of authors making changes to each issue\n",
    "unique_authors = changelog_cleaned.groupBy(\"id\").agg(spark_sum(when(col(\"author\") != \"\", 1).otherwise(0)).alias(\"unique_authors\"))\n",
    "\n",
    "# Join authors count to issues_final\n",
    "changelog_issues = changelog_issues.join(unique_authors, on=\"id\", how=\"left\")\n",
    "\n",
    "# Fill nulls with 0\n",
    "changelog_issues = changelog_issues.withColumn(\n",
    "    \"unique_authors\",\n",
    "    when(col(\"unique_authors\").isNull(), 0).otherwise(col(\"unique_authors\").cast(\"integer\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5adc3079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # issue lifespan\n",
    "# changelog_issues = changelog_issues.withColumn(\n",
    "#     \"workspan\",\n",
    "#     (col(\"last_change_at\").cast(\"long\") - col(\"first_change_at\").cast(\"long\")) / 86400  # Convert seconds to days\n",
    "# )\n",
    "# # Fill nulls with 0\n",
    "# changelog_issues = changelog_issues.withColumn(\n",
    "#     \"workspan\",\n",
    "#     when(col(\"workspan\").isNull(), 0).otherwise(col(\"workspan\").cast(\"double\"))\n",
    "# )\n",
    "\n",
    "# # note: the unit of workspan is days, not seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f550bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # work tempo indicator: mean time between changes\n",
    "# changelog_issues = changelog_issues.withColumn(\n",
    "#     \"mean_change_interval\",\n",
    "#     when(col(\"total_changes\") > 1, (col(\"workspan\") * 86400) / (col(\"total_changes\") - 1)).otherwise(0)\n",
    "# )\n",
    "# # Fill nulls with 0\n",
    "# changelog_issues = changelog_issues.withColumn(\n",
    "#     \"mean_change_interval\",\n",
    "#     when(col(\"mean_change_interval\").isNull(), 0).otherwise(col(\"mean_change_interval\").cast(\"double\"))\n",
    "# )\n",
    "\n",
    "# # note: the unit of mean_change_interval is seconds\n",
    "# # change to days\n",
    "# changelog_issues = changelog_issues.withColumn(\n",
    "#     \"mean_change_interval\",\n",
    "#     col(\"mean_change_interval\") / 86400  # Convert seconds to days\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2819ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # max inactivity gap: group by issue id and calculate the maximum gap between changes\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# # Do not mutate changelog_cleaned, use temporary columns\n",
    "# changelog_temp = changelog_cleaned.withColumn(\n",
    "#     \"_created_long\",\n",
    "#     col(\"created\").cast(\"long\")  # Convert to long for easier calculations\n",
    "# )\n",
    "# window_spec_inactivity = Window.partitionBy(\"id\").orderBy(\"_created_long\")\n",
    "# changelog_temp = changelog_temp.withColumn(\n",
    "#     \"_prev_created_long\",\n",
    "#     F.lag(\"_created_long\").over(window_spec_inactivity)\n",
    "# )\n",
    "# changelog_temp = changelog_temp.withColumn(\n",
    "#     \"_inactivity_gap\",\n",
    "#     when(col(\"_prev_created_long\").isNull(), 0).otherwise(col(\"_created_long\") - col(\"_prev_created_long\"))\n",
    "# )\n",
    "# changelog_issues = changelog_issues.join(\n",
    "#     changelog_temp.groupBy(\"id\").agg(F.max(\"_inactivity_gap\").alias(\"max_inactivity_gap\")),\n",
    "#     on=\"id\",\n",
    "#     how=\"left\"\n",
    "# )\n",
    "# # Fill nulls with 0\n",
    "# changelog_issues = changelog_issues.withColumn(\n",
    "#     \"max_inactivity_gap\",\n",
    "#     when(col(\"max_inactivity_gap\").isNull(), 0).otherwise(col(\"max_inactivity_gap\").cast(\"double\"))\n",
    "# )\n",
    "\n",
    "# # note: the unit of max_inactivity_gap is seconds\n",
    "# # change to days\n",
    "# changelog_issues = changelog_issues.withColumn(\n",
    "#     \"max_inactivity_gap\",\n",
    "#     col(\"max_inactivity_gap\") / 86400  # Convert seconds to days\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10a102df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      " |-- time_spent: string (nullable = true)\n",
      " |-- time_estimate: string (nullable = true)\n",
      " |-- original_estimate: string (nullable = true)\n",
      " |-- total_changes: integer (nullable = true)\n",
      " |-- status_changes: integer (nullable = true)\n",
      " |-- priority_changes: integer (nullable = true)\n",
      " |-- assignee_changes: integer (nullable = true)\n",
      " |-- issuetype_changes: integer (nullable = true)\n",
      " |-- resolution_changes: integer (nullable = true)\n",
      " |-- timeestimate_changes: integer (nullable = true)\n",
      " |-- description_changes: integer (nullable = true)\n",
      " |-- fixversion_changes: integer (nullable = true)\n",
      " |-- reopen_times: integer (nullable = true)\n",
      " |-- unique_authors: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "changelog_issues.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "613b69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 'count' column\n",
    "changelog_issues = changelog_issues.drop(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a716969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "\n",
    "# # Explicitly cast timestamp columns to string to avoid OSError on toPandas\n",
    "# timestamp_columns = ['last_change_at', 'first_change_at']\n",
    "# for col_name in timestamp_columns:\n",
    "#     changelog_issues = changelog_issues.withColumn(\n",
    "#         col_name,\n",
    "#         col(col_name).cast(\"string\")\n",
    "#     )\n",
    "\n",
    "changelog_issues.toPandas().to_csv(\"./apache/changelog_issues.csv\",index=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed7fffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
